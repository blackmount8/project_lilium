name: "Sentence Trans"

type: "embedding"
model_template: "sentence_transformers"

mixins: 
- STGenerationMixin

tokenizer_config:
  model_name_or_path: "/path/to/model"
  
generation_config: # TODO: Remove params
  return_dict_in_generate: true
  output_scores: false
  do_sample: true
  max_new_tokens: 32
  num_beams: 3

encode_config: # https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__
  return_token_type_ids: false
  truncation: true 
  padding: true
  max_length: 2048
  
decode_config: # https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode
  skip_special_tokens: true
  new_tokens_only: true

stream_config:
  stream_new_tokens: true
  stream_interval: 5
  skip_special_tokens: true
  stop_str: 
  - "<|endoftext|>"
  - "</s>"
  stop_token_ids: 